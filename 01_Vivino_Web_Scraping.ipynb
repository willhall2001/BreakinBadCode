{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vivino Webscraping \n",
    "This code automates the process of collecting data from Vivino, a popular platform for wine reviews and ratings. The scraper uses Selenium to navigate Vivino's dynamic content, extracting key details about wine bottles and storing them in an SQLite3 database for analysis. The goal is to create a structured dataset of information pertaining to each wine bottle, including the name, producer, wine type, vintage, price, region, country, ratings, and the number of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Notebook\n",
    "Begin by setting up the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vivino Data Storage \n",
    "Vivino imposes a limit of approximately 2000 bottles per filter search response. To scrape data for over 24,000 bottles listed on their home page, the filtering criteria needs to be incrementally adjsuted to create a list of URLs. Each URL should limit the number of bottles to approximately 2000, ensuring the scraping process adheres to the websites restraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of pages that are restricted to 2000 wines per page\n",
    "page_urls = ['https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=500&price_range_min=375&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=375&price_range_min=210&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=210&price_range_min=143&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=143&price_range_min=110&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=110&price_range_min=87&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=87&price_range_min=70&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=70&price_range_min=58&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=58&price_range_min=48&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=48&price_range_min=40&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=40&price_range_min=34&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=34&price_range_min=28&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=28&price_range_min=22&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            'https://www.vivino.com/explore?currency_code=CAD&min_rating=1&order_by=ratings_average&order=desc&page={page_num}&price_range_max=22&price_range_min=0&vc_only=&wsa_year=null&discount_prices=false&wine_type_ids[]=1',\n",
    "            \n",
    "]\n",
    "\n",
    "# Record the number of pages in each URL\n",
    "number_of_pages = [76, 73, 77, 72, 75, 67, 75, 76, 73, 76, 68, 77, 47]\n",
    "\n",
    "# Create an empty list to store URLs\n",
    "page_url_list = []\n",
    "\n",
    "# Loop through each URL template and corresponding number of pages\n",
    "for i in range(len(page_urls)):\n",
    "    for page_num in range(1, number_of_pages[i] + 1):\n",
    "        # Format the URL with the current page number\n",
    "        formatted_url = page_urls[i].format(page_num=page_num)\n",
    "        page_url_list.append(formatted_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping \n",
    "Now that the data is segmented into managable partions, the next step is to sequentially web scrape each URL in the generated list. The goal is to extract relevant information for each wine bottle, including the name, type, vintage, rating, price, and other available details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite3 Database for Web Scraping Storage\n",
    "To store such large amounts of data, an SQLite3 database was created. This database saves the data as a separate file on the local drive, making it easy to access, query, and update as needed. Additionally, it protects against data loss by storing already processed information, even if the scraping process is interrupted or crashes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQLite3 Database Creation\n",
    "The red_wines database is created with a unique constraint on the URL column. Each wine bottle on Vivino has a unique URL associated with it, which we can leverage to prevent duplicate entries in the database. This is especially useful when rerunning the code in the event of an interruption or crash, as it ensures that previously scraped data is not duplicated.\n",
    "\n",
    "Additionally, a scraper_state database is created to track and store the most recently processed URL from the page_url_list. This enables the scraper to resume from where it left off in case of interruptions, eliminating the need to rerun the entire scraping process from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database setup\n",
    "conn = sqlite3.connect('red_wines_final.db')  # Connect to SQLite database (or create it if it doesn't exist)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Create a table for storing wine data if it doesn't already exist\n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS red_wines (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        Producer TEXT,\n",
    "        WineType TEXT,\n",
    "        Year INTEGER,\n",
    "        Region TEXT,\n",
    "        Country TEXT,\n",
    "        URL TEXT UNIQUE,\n",
    "        Rating REAL, \n",
    "        Num_Ratings INTEGER,\n",
    "        Price REAL,\n",
    "        url_idx INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Create a table for storing the url pages state\n",
    "c.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS scraper_state (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        url_idx INTEGER,\n",
    "        last_url TEXT\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Check if there's a last URL to resume from\n",
    "c.execute('SELECT last_url FROM scraper_state WHERE id = 1')\n",
    "result = c.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Index Tracking\n",
    "In addition to the scraper state, we can track what URL has been processed wihtin the page_url_list to reference in case the program is inturrupted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the scraper has a saved state, resume from the last processed URL\n",
    "if result and result[0]:\n",
    "    # Find the index of the last processed URL in the URL list\n",
    "    next_url_idx = page_url_list.index(result[0])  # Index for the next URL to scrape\n",
    "    url_idx = page_url_list.index(result[0]) - 1   # Index for tracking the current URL being processed\n",
    "\n",
    "    # Set the starting URL to resume scraping\n",
    "    start_url = result[0]\n",
    "    print(f\"Resuming from last URL: {start_url}\")\n",
    "\n",
    "else:\n",
    "    # If no saved state exists, start scraping from the first URL in the list\n",
    "    next_url_idx = 0  # Start from the first URL\n",
    "    url_idx = -1      # No previous URL processed yet\n",
    "\n",
    "    # Set the starting URL to the first URL in the list\n",
    "    start_url = page_url_list[0]\n",
    "    print(\"Starting from the initial URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium for Web Scraping\n",
    "Many elements on Vivino's webpage are dynamically loaded using JavaScript, and Selenium effectively handles this dynamic content by waiting for elements to fully render before extracting data. Unlike static web scraping tools like BeautifulSoup, Selenium mimics the behavior of a real user by controlling a web browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WebDriver Options\n",
    "To speed up the web scraping process, we configure the WebDriver to disable image loading. This reduces page load times, which is particularly beneficial on image-heavy websites like Vivino. By optimizing the loading process, scraping becomes more efficient, especially when dealing with numerous pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup WebDriver options to not load images for faster loading\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element Extraction and HTML Parsing\n",
    "To collect the required data for each wine bottle on Vivino's webpage, the necessary HTML elements must be identified and extracted. Since Vivino relies heavily on dynamically loaded elements, Selenium can be used to load these elements. Methods such as find_element() and find_elements() can then efficiently extract information, including the wine's name, producer, type, ratings, location, and price. The following section outlines the key functions and the main scraping loop used to parse and store this data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Exraction\n",
    "Vivino uses multiple paths for displaying the price per bottle. The following functions are designed to handle this complexity by extracting numeric price values and accommodating two potential XPaths. The first function processes and cleans the price text to extract a numeric value, while the second function utilizes both a primary and secondary XPath to ensure reliable price retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract price \n",
    "def extract_price(price_text):\n",
    "    \"\"\"\n",
    "    Extracts a numeric price value from a string by cleaning and parsing it.\n",
    "\n",
    "    Arguments:\n",
    "        price_text (str): The text containing the price information to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        float or None: The extracted price as a float, or None if no numeric value is found.\n",
    "    \"\"\"\n",
    "    # Remove any non-numeric characters except for commas and decimal points\n",
    "    cleaned_text = re.sub(r'[^\\d.,]', '', price_text)\n",
    "    # Remove commas to convert the text into a proper float format\n",
    "    cleaned_text = cleaned_text.replace(',', '')\n",
    "    # Use regex to find the numeric pattern\n",
    "    match = re.search(r'\\d+(\\.\\d{1,2})?', cleaned_text)\n",
    "    return float(match.group()) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the price using primary and fallback XPaths\n",
    "def find_price(driver, index):\n",
    "    \"\"\"\n",
    "    Vivino has two possible XPaths to retrieve the price. This function finds the price using primary and fallback XPaths.\n",
    "\n",
    "    Arguments:\n",
    "        driver (WebDriver): The Selenium WebDriver instance controlling the browser.  \n",
    "        index (int): The zero-based index of the wine item for which the price is to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        float or None: The extracted price as a float or `None` if neither XPath works.\n",
    "    \"\"\"\n",
    "    # Primary XPath to use\n",
    "    primary_xpath = f\"(//div[contains(@class, 'addToCartButton__currency--2CTNX addToCartButton__prefix--3LzGf')]/following-sibling::div)[{index+1}]\"\n",
    "    # Fallback XPath to use if the first one fails\n",
    "    fallback_xpath = f\"(//div[contains(@class, 'addToCart__subText--1pvFt addToCart__ppcPrice--ydrd5')])[{index+1}]\"\n",
    "\n",
    "    try:\n",
    "        # Try using the primary XPath first\n",
    "        price_element = driver.find_element(By.XPATH, primary_xpath)\n",
    "        price_text = price_element.text\n",
    "        price = extract_price(price_text)\n",
    "        if price:\n",
    "            return price  # Return the price if a valid one is found\n",
    "    except Exception as e:\n",
    "        # If the primary XPath fails, print the error and move on to the fallback\n",
    "        print(f\"Primary XPath failed: {e}\")\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Use the fallback XPath if the primary one didn't work\n",
    "        price_element = driver.find_element(By.XPATH, fallback_xpath)\n",
    "        price_text = price_element.text\n",
    "        price = extract_price(price_text)\n",
    "        return price  # Return the price from the fallback XPath\n",
    "    except Exception as e:\n",
    "        # If both XPaths fail, print the error\n",
    "        print(f\"Fallback XPath also failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Type and Year Extraction\n",
    "Vivino stores the wine name and year in a single string (eg., 'Cabernet Sauvignon 2020'). To handle this, the following function is defined to separate the wine type and year from the given string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wine_type_and_year(wine_type_full):\n",
    "    \"\"\"\n",
    "    Extracts the wine type and year from a given wine type string.\n",
    "\n",
    "    If the wine type string contains a four-digit year (ex, \"Red Wine 2020\"), the function\n",
    "    separates the wine type and the year. If no year is found, the function sets the year to None.\n",
    "\n",
    "    Arguments:\n",
    "        wine_type_full (str): The full string containing the wine type and possibly the year.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the wine type (str) and the year (int or None).\n",
    "               Example: (\"Red Wine\", 2020) or (\"Red Wine\", None).\n",
    "    \"\"\"\n",
    "    # Use regex to find the wine type and a four-digit year at the end of the string\n",
    "    match = re.search(r'(.*?)(\\d{4})$', wine_type_full)\n",
    "    if match:\n",
    "        wine_type = match.group(1).strip()  # Extract the wine type\n",
    "        year = int(match.group(2))         # Extract the year and convert to an integer\n",
    "    else:\n",
    "        wine_type = wine_type_full         # Use the full string if no year is found\n",
    "        year = None                        # Set year to None\n",
    "\n",
    "    return wine_type, year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Scraping Loop\n",
    "This section contains the main scraping loop for collecting data from Vivino's explore page. The loop handles navigating through multiple pages, extracting wine details, and storing the data in a database while maintaining progress tracking for resumption in case of interruptions.\n",
    "\n",
    "The key steps for this function are defined as follows: \n",
    "1. Page Loading: Open and loads the page, using WebDriverWait to ensure all elements are present before extracting\n",
    "2. Data Extraction: Extracts information for each wine bottle and commits it to the database \n",
    "3. Pagination: Automatically navigates to the next URL in the page_url_list and updates the scraper_state to track the current URL index for resumption \n",
    "4. Error Handling: Skips over any problematic URL and logs its index for resumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Vivino explore page\n",
    "driver.get(start_url)\n",
    "\n",
    "# Allow the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Loop until there is no \"Next\" button or the URL does not change\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "        # Wait for the main elements to be present on the page\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//a[@data-testid='vintagePageLink']\"))\n",
    "        )\n",
    "        \n",
    "        wine_elements = driver.find_elements(By.XPATH, \"//a[@data-testid='vintagePageLink']\")\n",
    "        producers = driver.find_elements(By.XPATH, \"//div[contains(@class, 'wineInfoVintage__truncate--3QAtw')][1]\")\n",
    "        wine_types = driver.find_elements(By.XPATH, \"//div[contains(@class, 'wineInfoVintage__vintage--VvWlU wineInfoVintage__truncate--3QAtw')]\")\n",
    "        ratings = driver.find_elements(By.XPATH, \"//div[contains(@class, 'vivinoRating_averageValue__uDdPM')]\")\n",
    "        num_ratings = driver.find_elements(By.XPATH, \"//div[contains(@class, 'vivinoRating_caption__xL84P')]\")\n",
    "        locations = driver.find_elements(By.XPATH, \"//div[contains(@class, 'wineInfoLocation__regionAndCountry--1nEJz')]\")\n",
    "\n",
    "        url_idx += 1\n",
    "\n",
    "        # Extract data from the current page\n",
    "        for i in range(len(wine_elements)):\n",
    "            try:\n",
    "                # Extract elements needed for the database\n",
    "                url = wine_elements[i].get_attribute(\"href\")    # Extract the wine bottle's unique URL \n",
    "                producer = producers[i].text                    # Exract the producer name\n",
    "                wine_type_full = wine_types[i].text             # Extract the wine type name \n",
    "                rating_text = ratings[i].text                   # Extract the rating as a str\n",
    "                num_ratings_text = num_ratings[i].text          # Extract the number of ratings as a str\n",
    "                location = locations[i].text.split(\", \")        # Extract the location including country and region\n",
    "                                \n",
    "                # convert rating to number\n",
    "                rating = float(rating_text)\n",
    "\n",
    "                # Parse number of ratings by removing all characters that are not a digit\n",
    "                num_ratings_value = int(re.sub(r'\\D', '', num_ratings_text)) if num_ratings_text else None\n",
    "                \n",
    "                # Extract Region and Country if available\n",
    "                region = location[0].strip() if len(location) > 0 else None\n",
    "                country = location[1].strip() if len(location) > 1 else None\n",
    "\n",
    "                # Extract the year from the wine type, if present\n",
    "                wine_type, year = extract_wine_type_and_year(wine_type_full)\n",
    "                                    \n",
    "                # Find the price using the find_price function\n",
    "                price = find_price(driver, i)                \n",
    "                \n",
    "                # Insert into the database (avoid duplicates with UNIQUE constraint on URL)\n",
    "                c.execute('''\n",
    "                    INSERT OR IGNORE INTO red_wines (Producer, WineType, Year, Region, Country, URL, Rating, Num_Ratings, Price, url_idx)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ''', (producer, wine_type, year, region, country, url, rating, num_ratings_value, price, url_idx))\n",
    "\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Could not extract all data for a wine element: {e}\")\n",
    "\n",
    "        # Commit after each page to save progress\n",
    "        conn.commit()\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # update the ulr index\n",
    "            next_url_idx += 1            \n",
    "\n",
    "            # get the next url from the list\n",
    "            next_url = page_url_list[next_url_idx]\n",
    "\n",
    "            \n",
    "            if page_url_list[-1] == next_url:\n",
    "                break\n",
    "            \n",
    "            # Store the next_url in the database for resuming later\n",
    "            c.execute('''\n",
    "                INSERT OR REPLACE INTO scraper_state (id, url_idx, last_url)\n",
    "                VALUES (1, ?, ?)\n",
    "            ''', (url_idx, next_url,))\n",
    "            conn.commit()    \n",
    "\n",
    "            driver.get(next_url)\n",
    "\n",
    "            # Allow some time for the new page to load\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            # If there is an error navigating to the next URL\n",
    "            print(f\"Error navigating to the next URL: {e}. Last processed page index: {url_idx}\")\n",
    "            break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"No elements found, moving to next page: {e}. Skipped over url: {url_idx}\")\n",
    "        try:            \n",
    "            # update the ulr index\n",
    "            next_url_idx += 1            \n",
    "\n",
    "            # get the next url from the list\n",
    "            next_url = page_url_list[next_url_idx]\n",
    "\n",
    "            \n",
    "            if page_url_list[-1] == next_url:\n",
    "                break\n",
    "            \n",
    "            # Store the next_url in the database for resuming later\n",
    "            c.execute('''\n",
    "                INSERT OR REPLACE INTO scraper_state (id, url_idx, last_url)\n",
    "                VALUES (1, ?, ?)\n",
    "            ''', (url_idx, next_url,))\n",
    "            conn.commit()    \n",
    "\n",
    "            driver.get(next_url)\n",
    "\n",
    "            # Allow some time for the new page to load\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in main loop: {e}\")\n",
    "            break\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper State\n",
    "Lastly, if the code is rerun from the start, the scraper state needs to be cleared to let the code resume from the first URL in the page_url_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database setup\n",
    "conn = sqlite3.connect('red_wines_complete.db')  # Connect to SQLite database (or create it if it doesn't exist)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Add a reset flag\n",
    "RESET_SCRAPER_STATE = False  # Set to True to reset the scraper state\n",
    "\n",
    "if RESET_SCRAPER_STATE:\n",
    "    # Clear the scraper_state table\n",
    "    c.execute('DELETE FROM scraper_state')\n",
    "    conn.commit()\n",
    "    print(\"Scraper state has been reset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
